#!/usr/bin/env python3
"""
numeric_columns.py — Numeric Alignment Analysis (QA-only)
==========================================================

Groups numeric tokens into logical columns by:
  • Similar x-center or right-edge alignment
  • Consistent vertical spacing

These are NOT tables.  They are NOT rendered as grids.
They exist solely for:
  1. QA — detect anomalies (outlier values) within aligned columns
  2. Optional styling (highlight numeric alignment in review HTML)
  3. Alignment awareness for downstream consumers

No borders.  No grid.  No structure creation.

Usage:
    from numeric_columns import find_numeric_columns, detect_column_anomalies
"""

from __future__ import annotations

import re
import statistics
from collections import defaultdict
from dataclasses import dataclass, field
from typing import List, Optional

from token_extract import Token, PageTokens, YEAR_RE


# ────────────────────────────────────────────────────────────────────
#  Configuration
# ────────────────────────────────────────────────────────────────────
X_ALIGN_TOLERANCE   = 8.0      # pt — tokens within this x-range are same column
Y_GAP_MAX           = 40.0     # pt — max vertical gap between consecutive tokens
MIN_COLUMN_TOKENS   = 2        # minimum tokens to form a logical column
ANOMALY_MULTIPLIER  = 5.0      # value > median × N is an anomaly
LOW_CONFIDENCE_THR  = 0.7      # below this = low confidence token


# ────────────────────────────────────────────────────────────────────
#  Data Model
# ────────────────────────────────────────────────────────────────────
@dataclass
class NumericColumn:
    """A logical column of vertically-aligned numeric tokens."""
    column_id: int
    x_center: float             # average x-center of column
    x_right: float              # average right-edge of column
    tokens: List[Token] = field(default_factory=list)
    page_number: int = 0

    @property
    def count(self) -> int:
        return len(self.tokens)

    @property
    def values(self) -> List[Optional[float]]:
        """Parse numeric values from tokens (None if unparseable)."""
        return [parse_numeric(t.text) for t in self.tokens]


@dataclass
class ColumnAnomaly:
    """A numeric anomaly within a column."""
    column_id: int
    token: Token
    parsed_value: Optional[float]
    column_median: float
    reason: str                 # "outlier" | "low_confidence"


# ────────────────────────────────────────────────────────────────────
#  Numeric parsing
# ────────────────────────────────────────────────────────────────────
_ARABIC_DIGITS = str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789')


def parse_numeric(text: str) -> Optional[float]:
    """Best-effort parse of a numeric token to float."""
    s = text.strip()
    if not s:
        return None

    # Translate Arabic-Indic digits
    s = s.translate(_ARABIC_DIGITS)

    # Handle parenthesized negatives: (123) → -123
    neg = False
    if s.startswith('(') and s.endswith(')'):
        s = s[1:-1]
        neg = True

    # Remove thousands separators (commas, spaces)
    s = s.replace(',', '').replace('،', '').replace(' ', '')
    s = s.replace('−', '-').replace('–', '-')

    # Remove trailing %
    s = s.rstrip('%')

    try:
        val = float(s)
        return -val if neg else val
    except (ValueError, OverflowError):
        return None


# ────────────────────────────────────────────────────────────────────
#  Column Discovery
# ────────────────────────────────────────────────────────────────────
def find_numeric_columns(page_tokens: PageTokens) -> List[NumericColumn]:
    """
    Group numeric tokens into logical columns by alignment.

    Algorithm:
      1. Collect all NUMERIC tokens (excluding years, isolated labels).
      2. Sort by right-edge x position.
      3. Cluster by right-edge within X_ALIGN_TOLERANCE.
      4. Within each x-cluster, verify vertical continuity.
      5. Filter: minimum MIN_COLUMN_TOKENS per column.

    Right-edge alignment is preferred because financial numbers
    are typically right-aligned in columns.
    """
    # Collect numeric tokens, excluding year labels
    numeric_tokens = [
        t for t in page_tokens.tokens
        if t.token_type == "NUMERIC"
        and not _is_year_or_label(t.text)
        and len(t.text.strip()) > 0
    ]

    if len(numeric_tokens) < MIN_COLUMN_TOKENS:
        return []

    # Sort by right-edge
    numeric_tokens.sort(key=lambda t: t.bbox[2])

    # Cluster by right-edge x-position
    x_clusters: List[List[Token]] = []
    current_cluster = [numeric_tokens[0]]
    current_right = numeric_tokens[0].bbox[2]

    for tok in numeric_tokens[1:]:
        tok_right = tok.bbox[2]
        if abs(tok_right - current_right) <= X_ALIGN_TOLERANCE:
            current_cluster.append(tok)
            # Update running average
            current_right = sum(t.bbox[2] for t in current_cluster) / len(current_cluster)
        else:
            if len(current_cluster) >= MIN_COLUMN_TOKENS:
                x_clusters.append(current_cluster)
            current_cluster = [tok]
            current_right = tok_right

    if len(current_cluster) >= MIN_COLUMN_TOKENS:
        x_clusters.append(current_cluster)

    # Within each x-cluster, verify vertical continuity
    columns = []
    col_id = 0
    for cluster in x_clusters:
        # Sort by y-position (top to bottom)
        cluster.sort(key=lambda t: t.bbox[1])

        # Split if vertical gap is too large
        sub_columns = _split_by_vertical_gap(cluster)

        for sub in sub_columns:
            if len(sub) >= MIN_COLUMN_TOKENS:
                avg_x_center = sum((t.bbox[0] + t.bbox[2]) / 2 for t in sub) / len(sub)
                avg_x_right  = sum(t.bbox[2] for t in sub) / len(sub)
                columns.append(NumericColumn(
                    column_id=col_id,
                    x_center=round(avg_x_center, 2),
                    x_right=round(avg_x_right, 2),
                    tokens=sub,
                    page_number=page_tokens.page_number,
                ))
                col_id += 1

    return columns


def _split_by_vertical_gap(tokens: List[Token]) -> List[List[Token]]:
    """Split a list of vertically-sorted tokens at large Y gaps."""
    if len(tokens) <= 1:
        return [tokens]

    groups = [[tokens[0]]]
    for prev, cur in zip(tokens, tokens[1:]):
        gap = cur.bbox[1] - prev.bbox[3]
        if gap > Y_GAP_MAX:
            groups.append([cur])
        else:
            groups[-1].append(cur)

    return groups


def _is_year_or_label(text: str) -> bool:
    """Check if text is a year or a short isolated label (not data)."""
    stripped = text.strip()
    if not stripped:
        return True
    # Year: 2019, 2023, ٢٠٢٣, etc.
    if YEAR_RE.match(stripped):
        return True
    # Very short numeric labels (page numbers, section numbers)
    if len(stripped) <= 2 and stripped.isdigit():
        return True
    return False


# ────────────────────────────────────────────────────────────────────
#  Anomaly Detection (operates on columns, NOT tables)
# ────────────────────────────────────────────────────────────────────
def detect_column_anomalies(
    columns: List[NumericColumn],
    anomaly_multiplier: float = ANOMALY_MULTIPLIER,
    confidence_threshold: float = LOW_CONFIDENCE_THR,
) -> List[ColumnAnomaly]:
    """
    Detect anomalies within numeric columns:
      1. Value outliers — parsed value > median × multiplier
      2. Low confidence — OCR confidence below threshold

    Does NOT operate on tables.
    Ignores header years and isolated labels.
    """
    anomalies = []

    for col in columns:
        # Parse all values in the column
        parsed = [(tok, parse_numeric(tok.text)) for tok in col.tokens]
        valid_values = [v for _, v in parsed if v is not None and v != 0]

        if len(valid_values) < 2:
            continue

        median_val = statistics.median(valid_values)

        for tok, val in parsed:
            # Value outlier
            if val is not None and median_val != 0:
                ratio = abs(val) / abs(median_val) if median_val != 0 else 0
                if ratio > anomaly_multiplier:
                    anomalies.append(ColumnAnomaly(
                        column_id=col.column_id,
                        token=tok,
                        parsed_value=val,
                        column_median=median_val,
                        reason="outlier",
                    ))

            # Low confidence (scanned pages only)
            if tok.confidence < confidence_threshold:
                anomalies.append(ColumnAnomaly(
                    column_id=col.column_id,
                    token=tok,
                    parsed_value=val,
                    column_median=median_val,
                    reason="low_confidence",
                ))

    return anomalies


# ────────────────────────────────────────────────────────────────────
#  Summary & Serialization
# ────────────────────────────────────────────────────────────────────
def columns_summary(columns: List[NumericColumn]) -> dict:
    """Summary dict for logging."""
    return {
        "num_columns": len(columns),
        "total_numeric_tokens": sum(c.count for c in columns),
        "columns": [
            {
                "id": c.column_id,
                "x_right": c.x_right,
                "x_center": c.x_center,
                "count": c.count,
                "sample_values": [t.text for t in c.tokens[:3]],
            }
            for c in columns
        ],
    }


def anomalies_summary(anomalies: List[ColumnAnomaly]) -> dict:
    """Summary dict for logging."""
    return {
        "total_anomalies": len(anomalies),
        "outliers": sum(1 for a in anomalies if a.reason == "outlier"),
        "low_confidence": sum(1 for a in anomalies if a.reason == "low_confidence"),
        "details": [
            {
                "column": a.column_id,
                "text": a.token.text,
                "value": a.parsed_value,
                "median": a.column_median,
                "confidence": a.token.confidence,
                "reason": a.reason,
                "bbox": a.token.bbox,
            }
            for a in anomalies
        ],
    }


# ────────────────────────────────────────────────────────────────────
#  QA Review HTML (numeric columns highlighted, NOT tables)
# ────────────────────────────────────────────────────────────────────
def render_qa_html(
    all_page_tokens: List[PageTokens],
    all_columns: List[List[NumericColumn]],
    all_anomalies: List[List[ColumnAnomaly]],
) -> str:
    """
    Render a standalone HTML QA review page showing:
      • Each page's numeric columns (highlighted tokens)
      • Anomalies flagged with colored markers
      • No table grids, no borders
    """
    pages_html = []

    for page_tokens, columns, anomalies in zip(
        all_page_tokens, all_columns, all_anomalies
    ):
        pnum = page_tokens.page_number

        # Build anomaly lookup
        anomaly_set = set()
        for a in anomalies:
            key = (round(a.token.bbox[0], 1), round(a.token.bbox[1], 1),
                   a.token.text)
            anomaly_set.add(key)

        # Build column membership lookup
        col_membership = {}
        for col in columns:
            for tok in col.tokens:
                key = (round(tok.bbox[0], 1), round(tok.bbox[1], 1), tok.text)
                col_membership[key] = col.column_id

        page_html = f'<div class="page-section"><h2>Page {pnum}</h2>'
        page_html += f'<p>{len(columns)} numeric columns, '
        page_html += f'{sum(c.count for c in columns)} numeric tokens, '
        page_html += f'{len(anomalies)} anomalies</p>'

        if columns:
            page_html += '<div class="columns-list">'
            for col in columns:
                page_html += (
                    f'<div class="col-info">'
                    f'Col {col.column_id}: '
                    f'x_right={col.x_right:.0f}, '
                    f'{col.count} tokens — '
                    f'{", ".join(t.text for t in col.tokens[:5])}'
                    f'{"..." if col.count > 5 else ""}'
                    f'</div>'
                )
            page_html += '</div>'

        if anomalies:
            page_html += '<div class="anomaly-list">'
            for a in anomalies:
                cls = "outlier" if a.reason == "outlier" else "low-conf"
                page_html += (
                    f'<div class="anomaly {cls}">'
                    f'⚠ Col {a.column_id}: "{a.token.text}" '
                    f'(value={a.parsed_value}, median={a.column_median:.1f}, '
                    f'conf={a.token.confidence:.2f}) — {a.reason}'
                    f'</div>'
                )
            page_html += '</div>'

        page_html += '</div>'
        pages_html.append(page_html)

    body = "\n".join(pages_html)

    return f"""<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Numeric Column QA Review</title>
<style>
body {{ font-family: Arial, sans-serif; max-width: 900px; margin: 20px auto; }}
h1 {{ color: #333; border-bottom: 2px solid #007bff; padding-bottom: 8px; }}
h2 {{ color: #555; margin-top: 30px; }}
.page-section {{ margin-bottom: 30px; padding: 15px; border: 1px solid #ddd;
                  border-radius: 8px; background: #fafafa; }}
.columns-list {{ margin: 10px 0; }}
.col-info {{ padding: 4px 8px; margin: 2px 0; background: #e8f4f8;
             border-left: 3px solid #007bff; font-family: monospace; font-size: 13px; }}
.anomaly-list {{ margin: 10px 0; }}
.anomaly {{ padding: 6px 10px; margin: 3px 0; border-radius: 4px; font-size: 13px; }}
.anomaly.outlier {{ background: #fff3cd; border-left: 3px solid #ffc107; }}
.anomaly.low-conf {{ background: #f8d7da; border-left: 3px solid #dc3545; }}
</style>
</head>
<body>
<h1>Numeric Column QA Review</h1>
{body}
</body>
</html>"""
