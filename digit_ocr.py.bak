#!/usr/bin/env python3
"""
digit_ocr.py — Financial-Grade Digit-Level OCR
================================================

For every NumericRegion, performs:
  1. Crop region at 600 DPI from original page image
  2. Pre-process (binarize, denoise)
  3. Segment into individual digit glyphs via connected components
     + vertical projection
  4. OCR each glyph independently with TWO engines:
     - Surya OCR (primary — best Arabic-Indic support)
     - Tesseract Arabic (secondary — independent vote)
  5. Voting: if both agree → candidate digit;
            if disagree → UNTRUSTED
  6. Per-digit confidence and trust scoring

Numbers are data, not text.
Never guess digits. Never "correct" digits heuristically.
If certainty cannot be proven → UNTRUSTED.

Usage:
    from digit_ocr import ocr_numeric_region, DigitResult, RegionOCRResult
"""

from __future__ import annotations

import cv2
import numpy as np
from dataclasses import dataclass, field
from typing import List, Optional, Tuple
from enum import Enum

from numeric_region_detector import NumericRegion


# ────────────────────────────────────────────────────────────────────
#  Configuration
# ────────────────────────────────────────────────────────────────────
DIGIT_CROP_DPI       = 600
MIN_GLYPH_HEIGHT     = 8       # pixels — smaller than this is noise
MAX_GLYPH_HEIGHT_PCT = 0.9     # fraction of region height — larger is noise
MIN_GLYPH_AREA       = 25      # pixels² — minimum connected component area
CONFIDENCE_THRESHOLD = 0.60    # below this → UNTRUSTED
AGREEMENT_REQUIRED   = True    # dual-OCR agreement required for TRUSTED

# Tesseract config for single-digit recognition
TESS_DIGIT_CONFIG = '--psm 10 --oem 3 -l ara'  # PSM 10 = single character

# Arabic-Indic digit codepoints
ARABIC_INDIC_DIGITS = set('٠١٢٣٤٥٦٧٨٩')
EXTENDED_ARABIC_DIGITS = set('۰۱۲۳۴۵۶۷۸۹')  # Persian/Urdu (U+06F0–U+06F9)
WESTERN_DIGITS = set('0123456789')
ALL_DIGITS = ARABIC_INDIC_DIGITS | EXTENDED_ARABIC_DIGITS | WESTERN_DIGITS

# Mapping tables
WESTERN_TO_ARABIC = str.maketrans('0123456789', '٠١٢٣٤٥٦٧٨٩')
ARABIC_TO_WESTERN = str.maketrans('٠١٢٣٤٥٦٧٨٩', '0123456789')
EXTENDED_TO_ARABIC = str.maketrans('۰۱۲۳۴۵۶۷۸۹', '٠١٢٣٤٥٦٧٨٩')


# ────────────────────────────────────────────────────────────────────
#  Data Model
# ────────────────────────────────────────────────────────────────────
class TrustStatus(str, Enum):
    TRUSTED = "TRUSTED"
    UNTRUSTED = "UNTRUSTED"


class FailureReason(str, Enum):
    NONE = "NONE"
    OCR_DISAGREEMENT = "OCR_DISAGREEMENT"
    LOW_CONFIDENCE = "LOW_CONFIDENCE"
    VISUAL_MISMATCH = "VISUAL_MISMATCH"
    SEGMENTATION_FAILURE = "SEGMENTATION_FAILURE"
    EMPTY_RESULT = "EMPTY_RESULT"


@dataclass
class DigitResult:
    """OCR result for a single digit glyph."""
    glyph_image: np.ndarray = field(repr=False)
    predicted_digit: str        # Arabic-Indic digit (٠-٩) or '?'
    confidence: float           # 0.0–1.0
    bbox: List[int]             # [x0, y0, x1, y1] in region-crop pixels
    trust_status: TrustStatus = TrustStatus.UNTRUSTED
    failure_reason: FailureReason = FailureReason.NONE
    # Dual-OCR details
    surya_digit: str = ""
    surya_confidence: float = 0.0
    tesseract_digit: str = ""
    tesseract_confidence: float = 0.0


@dataclass
class RegionOCRResult:
    """Full OCR result for a numeric region."""
    region: NumericRegion
    digits: List[DigitResult] = field(default_factory=list)
    raw_text: str = ""          # concatenated digit predictions
    trust_score: float = 0.0    # aggregate 0.0–1.0
    trust_status: TrustStatus = TrustStatus.UNTRUSTED
    failure_reasons: List[FailureReason] = field(default_factory=list)

    def compute_trust(self):
        """Compute aggregate trust from per-digit results."""
        if not self.digits:
            self.trust_score = 0.0
            self.trust_status = TrustStatus.UNTRUSTED
            self.failure_reasons = [FailureReason.EMPTY_RESULT]
            return

        trusted = [d for d in self.digits
                   if d.trust_status == TrustStatus.TRUSTED]
        self.trust_score = len(trusted) / len(self.digits) if self.digits else 0
        self.raw_text = ''.join(d.predicted_digit for d in self.digits)

        reasons = set()
        for d in self.digits:
            if d.failure_reason != FailureReason.NONE:
                reasons.add(d.failure_reason)
        self.failure_reasons = sorted(reasons, key=lambda r: r.value)

        # A number is TRUSTED only if ALL digits are trusted
        if all(d.trust_status == TrustStatus.TRUSTED for d in self.digits):
            self.trust_status = TrustStatus.TRUSTED
        else:
            self.trust_status = TrustStatus.UNTRUSTED


# ────────────────────────────────────────────────────────────────────
#  Image Pre-processing
# ────────────────────────────────────────────────────────────────────
def _preprocess_for_digits(crop: np.ndarray) -> np.ndarray:
    """Pre-process a region crop for digit segmentation.

    Returns a clean binary image (white digits on black background).
    """
    if crop.size == 0:
        return np.zeros((1, 1), dtype=np.uint8)

    gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY) if crop.ndim == 3 else crop.copy()

    # CLAHE for contrast enhancement
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
    enhanced = clahe.apply(gray)

    # Bilateral filter to reduce noise while preserving edges
    denoised = cv2.bilateralFilter(enhanced, d=5, sigmaColor=50, sigmaSpace=50)

    # Otsu binarization
    _, binary = cv2.threshold(denoised, 0, 255,
                              cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

    # Small morphological close to connect broken strokes
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2))
    binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)

    return binary


# ────────────────────────────────────────────────────────────────────
#  Glyph Segmentation
# ────────────────────────────────────────────────────────────────────
@dataclass
class GlyphSegment:
    """A segmented glyph (potential digit) from a binary image."""
    image: np.ndarray       # cropped glyph image (grayscale)
    bbox: List[int]         # [x0, y0, x1, y1] in parent image pixels
    area: int
    center_x: float
    center_y: float


def _segment_glyphs(
    binary: np.ndarray,
    min_area: int = MIN_GLYPH_AREA,
    min_height: int = MIN_GLYPH_HEIGHT,
) -> List[GlyphSegment]:
    """Segment individual digit glyphs from a binary image.

    Uses connected components analysis with size/aspect filtering.
    Returns glyphs sorted left-to-right by x-position.
    """
    if binary.size == 0:
        return []

    img_h, img_w = binary.shape[:2]
    max_height = int(img_h * MAX_GLYPH_HEIGHT_PCT)

    # Connected components
    n_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(
        binary, connectivity=8)

    glyphs = []
    for i in range(1, n_labels):  # skip background
        area = stats[i, cv2.CC_STAT_AREA]
        x = stats[i, cv2.CC_STAT_LEFT]
        y = stats[i, cv2.CC_STAT_TOP]
        w = stats[i, cv2.CC_STAT_WIDTH]
        h = stats[i, cv2.CC_STAT_HEIGHT]

        if area < min_area:
            continue
        if h < min_height or h > max_height:
            continue
        if w == 0 or h == 0:
            continue

        # Extract glyph image with padding
        pad = 4
        x0 = max(0, x - pad)
        y0 = max(0, y - pad)
        x1 = min(img_w, x + w + pad)
        y1 = min(img_h, y + h + pad)

        glyph_img = binary[y0:y1, x0:x1].copy()

        glyphs.append(GlyphSegment(
            image=glyph_img,
            bbox=[x, y, x + w, y + h],
            area=area,
            center_x=centroids[i][0],
            center_y=centroids[i][1],
        ))

    # Sort left-to-right (Arabic-Indic digits read L→R even in RTL text)
    glyphs.sort(key=lambda g: g.center_x)

    return glyphs


def _merge_overlapping_glyphs(
    glyphs: List[GlyphSegment],
    x_overlap_threshold: float = 0.5,
) -> List[GlyphSegment]:
    """Merge vertically overlapping glyphs (diacritics, broken strokes).

    If two components overlap significantly in x, merge them into one.
    """
    if len(glyphs) <= 1:
        return glyphs

    merged = [glyphs[0]]
    for g in glyphs[1:]:
        prev = merged[-1]
        # Check x-overlap
        overlap_x0 = max(prev.bbox[0], g.bbox[0])
        overlap_x1 = min(prev.bbox[2], g.bbox[2])
        overlap_w = max(0, overlap_x1 - overlap_x0)

        prev_w = prev.bbox[2] - prev.bbox[0]
        g_w = g.bbox[2] - g.bbox[0]
        min_w = min(prev_w, g_w)

        if min_w > 0 and overlap_w / min_w > x_overlap_threshold:
            # Merge: expand bbox and combine
            new_x0 = min(prev.bbox[0], g.bbox[0])
            new_y0 = min(prev.bbox[1], g.bbox[1])
            new_x1 = max(prev.bbox[2], g.bbox[2])
            new_y1 = max(prev.bbox[3], g.bbox[3])
            merged[-1] = GlyphSegment(
                image=prev.image,  # will be re-cropped later
                bbox=[new_x0, new_y0, new_x1, new_y1],
                area=prev.area + g.area,
                center_x=(prev.center_x + g.center_x) / 2,
                center_y=(prev.center_y + g.center_y) / 2,
            )
        else:
            merged.append(g)

    return merged


# ────────────────────────────────────────────────────────────────────
#  Surya OCR for single digit
# ────────────────────────────────────────────────────────────────────
_surya_loaded = False
_surya_foundation = None
_surya_rec = None
_surya_det = None


def _load_surya_models():
    """Load Surya models (singleton)."""
    global _surya_loaded, _surya_foundation, _surya_rec, _surya_det
    if _surya_loaded:
        return
    from surya.foundation import FoundationPredictor
    from surya.recognition import RecognitionPredictor
    from surya.detection import DetectionPredictor
    _surya_foundation = FoundationPredictor(device="cpu")
    _surya_det = DetectionPredictor(device="cpu")
    _surya_rec = RecognitionPredictor(_surya_foundation)
    _surya_loaded = True


def _surya_ocr_glyph(glyph_img: np.ndarray) -> Tuple[str, float]:
    """OCR a single glyph image with Surya.

    Returns (predicted_char, confidence).
    """
    _load_surya_models()
    from PIL import Image

    # Convert binary to RGB for Surya
    if glyph_img.ndim == 2:
        rgb = cv2.cvtColor(glyph_img, cv2.COLOR_GRAY2RGB)
    else:
        rgb = cv2.cvtColor(glyph_img, cv2.COLOR_BGR2RGB)

    # Invert if needed (Surya expects dark text on light background)
    if np.mean(rgb) < 128:
        rgb = 255 - rgb

    # Scale up small glyphs
    h, w = rgb.shape[:2]
    if h < 32:
        scale = 48 / max(h, 1)
        rgb = cv2.resize(rgb, None, fx=scale, fy=scale,
                         interpolation=cv2.INTER_CUBIC)

    # Add white padding
    pad = 16
    padded = cv2.copyMakeBorder(rgb, pad, pad, pad, pad,
                                cv2.BORDER_CONSTANT, value=(255, 255, 255))

    pil_img = Image.fromarray(padded)

    try:
        results = _surya_rec(
            [pil_img],
            task_names=["ocr_with_boxes"],
            det_predictor=_surya_det,
            return_words=True,
            sort_lines=True,
        )
        if results and results[0].text_lines:
            text = results[0].text_lines[0].text.strip()
            conf = results[0].text_lines[0].confidence or 0.0
            # Extract just the first digit character
            for ch in text:
                if ch in ARABIC_INDIC_DIGITS or ch in WESTERN_DIGITS:
                    # Normalize to Arabic-Indic
                    digit = ch.translate(WESTERN_TO_ARABIC) if ch in WESTERN_DIGITS else ch
                    return digit, conf
            # If no digit found, return raw
            return text[:1] if text else '?', conf * 0.5
        return '?', 0.0
    except Exception:
        return '?', 0.0


# ────────────────────────────────────────────────────────────────────
#  Tesseract OCR for single digit
# ────────────────────────────────────────────────────────────────────
def _tesseract_ocr_glyph(glyph_img: np.ndarray) -> Tuple[str, float]:
    """OCR a single glyph image with Tesseract Arabic.

    Returns (predicted_char, confidence).
    """
    try:
        import pytesseract
        from PIL import Image
    except ImportError:
        return '?', 0.0

    # Convert binary to grayscale for Tesseract
    if glyph_img.ndim == 2:
        gray = glyph_img.copy()
    else:
        gray = cv2.cvtColor(glyph_img, cv2.COLOR_BGR2GRAY)

    # Invert if needed (Tesseract expects dark text on light background)
    if np.mean(gray) < 128:
        gray = 255 - gray

    # Scale up
    h, w = gray.shape[:2]
    if h < 48:
        scale = 64 / max(h, 1)
        gray = cv2.resize(gray, None, fx=scale, fy=scale,
                          interpolation=cv2.INTER_CUBIC)

    # Add white padding
    pad = 20
    padded = cv2.copyMakeBorder(gray, pad, pad, pad, pad,
                                cv2.BORDER_CONSTANT, value=255)

    pil_img = Image.fromarray(padded)

    try:
        # PSM 10 = single character
        data = pytesseract.image_to_data(
            pil_img, config=TESS_DIGIT_CONFIG,
            output_type=pytesseract.Output.DICT)

        best_char = '?'
        best_conf = 0.0

        for i, text in enumerate(data['text']):
            text = text.strip()
            if not text:
                continue
            conf = int(data['conf'][i])
            if conf < 0:
                continue

            # Check if it's a digit
            for ch in text:
                if ch in ARABIC_INDIC_DIGITS:
                    if conf / 100.0 > best_conf:
                        best_char = ch
                        best_conf = conf / 100.0
                elif ch in WESTERN_DIGITS:
                    digit = ch.translate(WESTERN_TO_ARABIC)
                    if conf / 100.0 > best_conf:
                        best_char = digit
                        best_conf = conf / 100.0

        return best_char, best_conf

    except Exception:
        return '?', 0.0


# ────────────────────────────────────────────────────────────────────
#  Dual-OCR Voting
# ────────────────────────────────────────────────────────────────────
def _normalize_digit(ch: str) -> str:
    """Normalize a digit to Arabic-Indic for comparison."""
    if ch in WESTERN_DIGITS:
        return ch.translate(WESTERN_TO_ARABIC)
    if ch in EXTENDED_ARABIC_DIGITS:
        return ch.translate(EXTENDED_TO_ARABIC)
    return ch


def _vote_digit(
    surya_digit: str,
    surya_conf: float,
    tess_digit: str,
    tess_conf: float,
) -> Tuple[str, float, TrustStatus, FailureReason]:
    """Dual-OCR vote for a single digit.

    Rules:
      - Both agree + both confident → TRUSTED
      - Both agree + one low confidence → TRUSTED (lower score)
      - Disagree → UNTRUSTED (use higher-confidence result)
      - Either is '?' → fallback to the other

    Returns (digit, confidence, status, reason).
    """
    s_norm = _normalize_digit(surya_digit)
    t_norm = _normalize_digit(tess_digit)

    # Both unknown
    if s_norm == '?' and t_norm == '?':
        return '?', 0.0, TrustStatus.UNTRUSTED, FailureReason.EMPTY_RESULT

    # One unknown — use the other but lower trust
    if s_norm == '?':
        status = (TrustStatus.TRUSTED if tess_conf >= CONFIDENCE_THRESHOLD
                  else TrustStatus.UNTRUSTED)
        reason = FailureReason.NONE if status == TrustStatus.TRUSTED else FailureReason.LOW_CONFIDENCE
        return t_norm, tess_conf * 0.8, status, reason

    if t_norm == '?':
        # Tesseract failed — rely on Surya alone with reduced trust
        status = (TrustStatus.TRUSTED if surya_conf >= CONFIDENCE_THRESHOLD
                  else TrustStatus.UNTRUSTED)
        reason = FailureReason.NONE if status == TrustStatus.TRUSTED else FailureReason.LOW_CONFIDENCE
        return s_norm, surya_conf * 0.9, status, reason

    # Both have results — check agreement
    if s_norm in ARABIC_INDIC_DIGITS and t_norm in ARABIC_INDIC_DIGITS:
        if s_norm == t_norm:
            # Agreement!
            combined_conf = (surya_conf + tess_conf) / 2
            if combined_conf >= CONFIDENCE_THRESHOLD:
                return s_norm, combined_conf, TrustStatus.TRUSTED, FailureReason.NONE
            else:
                return s_norm, combined_conf, TrustStatus.UNTRUSTED, FailureReason.LOW_CONFIDENCE
        else:
            # Disagreement — use higher confidence but mark UNTRUSTED
            if surya_conf >= tess_conf:
                return s_norm, surya_conf * 0.7, TrustStatus.UNTRUSTED, FailureReason.OCR_DISAGREEMENT
            else:
                return t_norm, tess_conf * 0.7, TrustStatus.UNTRUSTED, FailureReason.OCR_DISAGREEMENT

    # One is a digit, other is not
    if s_norm in ARABIC_INDIC_DIGITS:
        return s_norm, surya_conf * 0.8, TrustStatus.UNTRUSTED, FailureReason.OCR_DISAGREEMENT
    if t_norm in ARABIC_INDIC_DIGITS:
        return t_norm, tess_conf * 0.8, TrustStatus.UNTRUSTED, FailureReason.OCR_DISAGREEMENT

    # Neither is a recognized digit
    return '?', 0.0, TrustStatus.UNTRUSTED, FailureReason.EMPTY_RESULT


# ────────────────────────────────────────────────────────────────────
#  Token-Level Dual OCR (practical approach)
# ────────────────────────────────────────────────────────────────────
def ocr_token_dual(
    token_crop: np.ndarray,
    token_bbox_px: List[int],
    original_text: str = "",
    original_confidence: float = 0.0,
) -> Tuple[str, float, TrustStatus, List[FailureReason]]:
    """Validate a numeric token using dual-OCR strategy.

    IMPORTANT: Uses the ORIGINAL full-page Surya OCR result as the
    primary signal (Surya fails on small crops — it needs full-page
    context). Only runs Tesseract on the crop as a secondary check.

    If Tesseract's digit sequence matches the original → agreement bonus.
    If Tesseract disagrees or returns garbage → rely on original + lower trust.

    Returns (text, confidence, status, failure_reasons).
    """
    from PIL import Image

    if token_crop.size == 0:
        return '?', 0.0, TrustStatus.UNTRUSTED, [FailureReason.EMPTY_RESULT]

    # Extract digit sequence from original Surya text
    def extract_digits(text):
        return ''.join(_normalize_digit(c) for c in text
                       if c in ARABIC_INDIC_DIGITS or c in WESTERN_DIGITS
                       or c in EXTENDED_ARABIC_DIGITS)

    surya_digits = extract_digits(original_text)
    surya_conf = original_confidence

    # --- Tesseract OCR on crop (secondary validation) ---
    gray = cv2.cvtColor(token_crop, cv2.COLOR_BGR2GRAY) if token_crop.ndim == 3 else token_crop.copy()
    if np.mean(gray) < 128:
        gray = 255 - gray

    # Scale up small crops for Tesseract
    h, w = gray.shape[:2]
    if h < 48:
        scale = 64 / max(h, 1)
        gray = cv2.resize(gray, None, fx=scale, fy=scale,
                          interpolation=cv2.INTER_CUBIC)

    pad = 16
    padded = cv2.copyMakeBorder(gray, pad, pad, pad, pad,
                                cv2.BORDER_CONSTANT, value=255)

    tess_text = ''
    tess_conf = 0.0
    try:
        import pytesseract
        pil_gray = Image.fromarray(padded)
        data = pytesseract.image_to_data(
            pil_gray, config='--psm 7 --oem 3 -l ara',
            output_type=pytesseract.Output.DICT)
        texts, confs = [], []
        for i, t in enumerate(data['text']):
            t = t.strip()
            if t:
                texts.append(t)
                c = int(data['conf'][i])
                confs.append(c / 100.0 if c > 0 else 0.0)
        tess_text = ' '.join(texts)
        tess_conf = sum(confs) / len(confs) if confs else 0.0
    except Exception:
        pass

    tess_digits = extract_digits(tess_text)

    # --- Voting Logic ---
    # Primary: original Surya (high quality, full-page context)
    # Secondary: Tesseract on crop (noisy, often fails on Arabic-Indic)

    if not surya_digits:
        # Original OCR had no digits — unusual for a NUMERIC token
        if tess_digits:
            return tess_digits, tess_conf * 0.5, TrustStatus.UNTRUSTED, [FailureReason.LOW_CONFIDENCE]
        return '?', 0.0, TrustStatus.UNTRUSTED, [FailureReason.EMPTY_RESULT]

    if not tess_digits:
        # Tesseract returned no digits (common for Arabic-Indic)
        # Trust original based on its confidence
        status = TrustStatus.TRUSTED if surya_conf >= CONFIDENCE_THRESHOLD else TrustStatus.UNTRUSTED
        reason = [] if status == TrustStatus.TRUSTED else [FailureReason.LOW_CONFIDENCE]
        return surya_digits, surya_conf, status, reason

    # Both have digits — compare
    if surya_digits == tess_digits:
        # Agreement! Both engines see the same digits → high trust
        combined = max(surya_conf, (surya_conf + tess_conf) / 2)
        return surya_digits, combined, TrustStatus.TRUSTED, []
    else:
        # Disagreement — assess whether Tesseract's opinion is credible
        # If Surya confidence significantly exceeds Tesseract, trust Surya
        # (Tesseract often returns garbage on Arabic-Indic small crops)
        TESS_CREDIBILITY_THRESHOLD = 0.60  # Tesseract needs this conf to count
        surya_dominance = surya_conf - tess_conf

        if tess_conf < TESS_CREDIBILITY_THRESHOLD or surya_dominance > 0.20:
            # Tesseract is low-confidence or Surya is much more confident
            # → trust Surya, mild penalty
            status = TrustStatus.TRUSTED if surya_conf >= CONFIDENCE_THRESHOLD else TrustStatus.UNTRUSTED
            return surya_digits, surya_conf * 0.90, status, []
        else:
            # Tesseract is credible and disagrees → genuine uncertainty
            return surya_digits, surya_conf * 0.70, TrustStatus.UNTRUSTED, [FailureReason.OCR_DISAGREEMENT]


# ────────────────────────────────────────────────────────────────────
#  Per-Token OCR with Trust (main entry point for pipeline)
# ────────────────────────────────────────────────────────────────────
@dataclass
class TokenOCRResult:
    """OCR result for a single numeric token with trust scoring."""
    original_text: str          # text from primary OCR (Surya)
    validated_text: str         # text after dual-OCR voting
    surya_text: str = ""
    tesseract_text: str = ""
    surya_confidence: float = 0.0
    tesseract_confidence: float = 0.0
    trust_score: float = 0.0
    trust_status: TrustStatus = TrustStatus.UNTRUSTED
    dual_agreed: bool = False   # True only when both engines return same digits
    failure_reasons: List[FailureReason] = field(default_factory=list)
    bbox: List[float] = field(default_factory=list)  # PDF points


def ocr_numeric_token(
    full_page_image: np.ndarray,
    token_bbox_pt: List[float],
    page_width: float,
    page_height: float,
    original_text: str = "",
    original_confidence: float = 0.0,
) -> TokenOCRResult:
    """Validate a single numeric token using dual-OCR.

    Uses the ORIGINAL full-page Surya OCR result as primary signal
    and runs Tesseract on the crop as secondary validation.

    Args:
        full_page_image: Full page at high DPI (BGR).
        token_bbox_pt: Token bbox in PDF points [x0, y0, x1, y1].
        page_width: Page width in points.
        page_height: Page height in points.
        original_text: Text from primary OCR pass (Surya full-page).
        original_confidence: Confidence from primary OCR pass.

    Returns:
        TokenOCRResult with trust scoring.
    """
    img_h, img_w = full_page_image.shape[:2]
    scale_x = img_w / page_width
    scale_y = img_h / page_height

    # Crop token region with padding
    pad_px = 12
    x0 = max(0, int(token_bbox_pt[0] * scale_x) - pad_px)
    y0 = max(0, int(token_bbox_pt[1] * scale_y) - pad_px)
    x1 = min(img_w, int(token_bbox_pt[2] * scale_x) + pad_px)
    y1 = min(img_h, int(token_bbox_pt[3] * scale_y) + pad_px)

    crop = full_page_image[y0:y1, x0:x1]

    if crop.size == 0:
        return TokenOCRResult(
            original_text=original_text,
            validated_text=original_text,
            surya_text=original_text,
            surya_confidence=original_confidence,
            trust_status=TrustStatus.UNTRUSTED,
            failure_reasons=[FailureReason.SEGMENTATION_FAILURE],
            bbox=token_bbox_pt,
        )

    # Run dual OCR (uses original Surya result + Tesseract on crop)
    text, conf, status, reasons = ocr_token_dual(
        crop, [x0, y0, x1, y1],
        original_text=original_text,
        original_confidence=original_confidence,
    )

    # Determine if dual-OCR agreed (both engines returned same digits)
    # Agreement only when status is TRUSTED AND no OCR_DISAGREEMENT
    agreed = (status == TrustStatus.TRUSTED and
              FailureReason.OCR_DISAGREEMENT not in reasons and
              FailureReason.EMPTY_RESULT not in reasons and
              FailureReason.LOW_CONFIDENCE not in reasons)

    return TokenOCRResult(
        original_text=original_text,
        validated_text=text if text and text != '?' else original_text,
        surya_text=original_text,
        tesseract_text="",
        surya_confidence=original_confidence,
        tesseract_confidence=0.0,
        trust_score=conf,
        trust_status=status,
        dual_agreed=agreed,
        failure_reasons=reasons,
        bbox=token_bbox_pt,
    )


# ────────────────────────────────────────────────────────────────────
#  Batch OCR for a page's numeric tokens
# ────────────────────────────────────────────────────────────────────
def ocr_page_numeric_tokens(
    page_tokens,  # PageTokens
    full_page_image: np.ndarray,
) -> List[TokenOCRResult]:
    """Run dual-OCR validation on all NUMERIC tokens from a page.

    Uses original Surya full-page OCR results as primary signal
    and Tesseract on crops as secondary validation.

    Args:
        page_tokens: PageTokens with extracted tokens.
        full_page_image: Full page rendered at DIGIT_CROP_DPI (BGR).

    Returns:
        List of TokenOCRResult for each numeric token.
    """
    results = []
    numeric_tokens = [t for t in page_tokens.tokens
                      if t.token_type == "NUMERIC"]

    for tok in numeric_tokens:
        result = ocr_numeric_token(
            full_page_image=full_page_image,
            token_bbox_pt=tok.bbox,
            page_width=page_tokens.page_width,
            page_height=page_tokens.page_height,
            original_text=tok.text,
            original_confidence=tok.confidence,
        )
        results.append(result)

    return results
